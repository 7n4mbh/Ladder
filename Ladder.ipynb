{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:10000] / 255.0, X_train_full[10000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:10000], y_train_full[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Denoising(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_bn = keras.layers.BatchNormalization()\n",
    "        \n",
    "    def build(self, batch_input_shape):\n",
    "        print(\"Denoising.build(): batch_input_shape = \", batch_input_shape)\n",
    "        z_tilda, _u = batch_input_shape\n",
    "        units = z_tilda[-1]\n",
    "        print(\"Denoising.build(): batch_input_shape = \", batch_input_shape, \"units = \", units)\n",
    "        self.a_1 = self.add_weight(name = \"a_1\", shape = [units], initializer = \"lecun_normal\")\n",
    "        self.a_2 = self.add_weight(name = \"a_2\", shape = [units], initializer = \"lecun_normal\")\n",
    "        self.a_3 = self.add_weight(name = \"a_3\", shape = [units], initializer = \"lecun_normal\")\n",
    "        self.a_4 = self.add_weight(name = \"a_4\", shape = [units], initializer = \"lecun_normal\")\n",
    "        self.a_5 = self.add_weight(name = \"a_5\", shape = [units], initializer = \"lecun_normal\")\n",
    "        self.a_6 = self.add_weight(name = \"a_6\", shape = [units], initializer = \"lecun_normal\")\n",
    "        self.a_7 = self.add_weight(name = \"a_7\", shape = [units], initializer = \"lecun_normal\")\n",
    "        self.a_8 = self.add_weight(name = \"a_8\", shape = [units], initializer = \"lecun_normal\")\n",
    "        self.a_9 = self.add_weight(name = \"a_9\", shape = [units], initializer = \"lecun_normal\")\n",
    "        self.a_10 = self.add_weight(name = \"a_10\", shape = [units], initializer = \"lecun_normal\")        \n",
    "        super().build(batch_input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        print(\"Denoising.call(): inputs = \", inputs, \"self.a_1 = \", self.a_1)\n",
    "        z_tilda, _u = inputs\n",
    "        u = self.hidden_bn(_u)\n",
    "        mu = tf.math.multiply(self.a_1, tf.math.multiply(self.a_2, u) + self.a_3) + tf.math.multiply(self.a_4, u) + self.a_5\n",
    "        v = tf.math.multiply(self.a_6, tf.math.multiply(self.a_7, u) + self.a_8) + tf.math.multiply(self.a_9, u) + self.a_10\n",
    "        z_hat = tf.math.multiply(z_tilda - mu, v) + mu\n",
    "        \n",
    "        return z_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClassifier4(keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [\n",
    "            keras.layers.Flatten(input_shape = [28, 28]),\n",
    "            keras.layers.Dense(1000, kernel_initializer = \"lecun_normal\"),\n",
    "            keras.layers.Dense(500, kernel_initializer = \"lecun_normal\"),\n",
    "            keras.layers.Dense(250, kernel_initializer = \"lecun_normal\"),\n",
    "            keras.layers.Dense(250, kernel_initializer = \"lecun_normal\"),\n",
    "            keras.layers.Dense(250, kernel_initializer = \"lecun_normal\"),\n",
    "        ]\n",
    "        self.bn = [keras.layers.BatchNormalization() for _ in range(len(self.hidden))]\n",
    "        self.out =  keras.layers.Dense(10, kernel_initializer = \"lecun_normal\")\n",
    "\n",
    "        self.hidden_corrupted = [\n",
    "            keras.layers.Flatten(input_shape = [28, 28]),\n",
    "            keras.layers.Dense(1000, kernel_initializer = \"lecun_normal\"),\n",
    "            keras.layers.Dense(500, kernel_initializer = \"lecun_normal\"),\n",
    "            keras.layers.Dense(250, kernel_initializer = \"lecun_normal\"),\n",
    "            keras.layers.Dense(250, kernel_initializer = \"lecun_normal\"),\n",
    "            keras.layers.Dense(250, kernel_initializer = \"lecun_normal\"),\n",
    "        ]\n",
    "        self.bn_corrupted = [keras.layers.BatchNormalization() for _ in range(len(self.hidden))]\n",
    "        self.out_corrupted =  keras.layers.Dense(10, kernel_initializer = \"lecun_normal\")\n",
    "        \n",
    "        self.noise = keras.layers.GaussianNoise(stddev = 0.3)\n",
    "        \n",
    "        self.hidden_decoder = [\n",
    "            keras.layers.Dense(784, kernel_initializer = \"lecun_normal\"),\n",
    "            keras.layers.Dense(1000, kernel_initializer = \"lecun_normal\"),\n",
    "            keras.layers.Dense(500, kernel_initializer = \"lecun_normal\"),\n",
    "            keras.layers.Dense(250, kernel_initializer = \"lecun_normal\"),\n",
    "            keras.layers.Dense(250, kernel_initializer = \"lecun_normal\"),\n",
    "            keras.layers.Dense(250, kernel_initializer = \"lecun_normal\")\n",
    "        ]\n",
    "        self.hidden_denoising = [Denoising() for _ in range(len(self.hidden_decoder))]\n",
    "        self.in_decoder = Denoising()\n",
    "        \n",
    "    def build(self, batch_input_shape):\n",
    "        super().build(batch_input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Corrupted encoder and classifier\n",
    "        z_tilda = []\n",
    "        h_tilda = []    \n",
    "        for i, layer in enumerate(self.hidden_corrupted):\n",
    "            if i > 0:\n",
    "                bn = self.bn_corrupted[i - 1]\n",
    "                z_tilda.append(self.noise(bn(layer(h_tilda[-1]))))\n",
    "                h_tilda.append(tf.nn.relu(z_tilda[-1]))\n",
    "            else:\n",
    "                z_tilda.append(self.noise(layer(inputs)))\n",
    "                h_tilda.append(z_tilda[-1])\n",
    "                \n",
    "        layer = self.out_corrupted\n",
    "        bn = self.bn_corrupted[-1]\n",
    "        z_tilda.append(bn(layer(h_tilda[-1])))\n",
    "        h_tilda.append(tf.nn.softmax(z_tilda[-1]))\n",
    "        \n",
    "        # Clean encoder (for denoising targets)\n",
    "        z = []\n",
    "        h = []\n",
    "        mu = []\n",
    "        sigma = []\n",
    "        for i, layer in enumerate(self.hidden):\n",
    "            if i > 0:\n",
    "                bn = self.bn[i - 1]\n",
    "                z.append(bn(layer(h[-1])))\n",
    "                h.append(tf.nn.relu(z[-1]))\n",
    "                mu.append(bn.weights[2])\n",
    "                sigma.append(bn.weights[3])\n",
    "            else:\n",
    "                z.append(layer(inputs))\n",
    "                h.append(z[-1])\n",
    "                mu.append(np.zeros(h[-1].shape[-1]))\n",
    "                sigma.append(np.ones(h[-1].shape[-1]))\n",
    "        \n",
    "        layer = self.out\n",
    "        bn = self.bn[-1]\n",
    "        z.append(bn(layer(h[-1])))\n",
    "        h.append(tf.nn.softmax(z[-1]))\n",
    "        mu.append(bn.weights[2])\n",
    "        sigma.append(bn.weights[3])        \n",
    "\n",
    "        # Decoder and denoising\n",
    "        z_hat = [None for _ in range(len(h))]\n",
    "        z_hat_BN = [None for _ in range(len(h))]\n",
    "        \n",
    "        for l in reversed(range(len(h))):\n",
    "            if l == len(h) - 1:\n",
    "                z_hat[l] = self.in_decoder((z_tilda[l], h[l]))           \n",
    "            else:\n",
    "                _u = self.hidden_decoder[l](z_hat[l + 1])\n",
    "                z_hat[l] = self.hidden_denoising[l]((z_tilda[l], _u))\n",
    "        #    \n",
    "            z_hat_BN[l] = (z_hat[l] - mu[l]) / sigma[l]\n",
    "        \n",
    "       \n",
    "        # Cost function C for training\n",
    "        #print(\"z = \", z, \"z_hat = \", z_hat)\n",
    "        #print(\"z - z_hat = \", tf.math.subtract(z, z_hat_BN))\n",
    "        #for i in range(len(z)):\n",
    "            #print(\"i = \", i, \"z[i] = \", z[i], \"z_hat_BN = \", z_hat_BN[i])\n",
    "            #print(\"z - z_hat = \", tf.reduce_sum(tf.math.square(z[i] - z_hat_BN[i]))) # tf.math.subtract(z[i], z_hat_BN[i]))\n",
    "            #fn = lambda a, b: tf.reduce_sum(tf.math.square(a - b))\n",
    "            #self.add_loss(fn(z[i], z_hat_BN[i]))\n",
    "            \n",
    "        #self.add_loss(lambda: 0.1)\n",
    "        #self.add_loss(tf.reduce_mean(err)) #[1000, 10, 0.1, 0.1, 0.1, 0.1, 0.1]))) #lambda_lはベット定義\n",
    "        \n",
    "        \n",
    "        #return h[-1]\n",
    "        return [h[-1]] + z + z_hat_BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossfunc_ladder_supervised(y_true, y_pred):\n",
    "    print(\"lossfunc_ladder_supervised: y_true = \", y_true, \", y_pred = \", y_pred)\n",
    "    h, z, z_hat_BN = y_pred\n",
    "    loss = keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denoising.build(): batch_input_shape =  (TensorShape([None, 10]), TensorShape([None, 10]))\n",
      "Denoising.build(): batch_input_shape =  (TensorShape([None, 10]), TensorShape([None, 10])) units =  10\n",
      "Denoising.call(): inputs =  (<tf.Tensor 'batch_normalization_1035/batchnorm/add_1:0' shape=(None, 10) dtype=float32>, <tf.Tensor 'Softmax_1:0' shape=(None, 10) dtype=float32>) self.a_1 =  <tf.Variable 'denoising_377/a_1:0' shape=(10,) dtype=float32>\n",
      "Denoising.build(): batch_input_shape =  (TensorShape([None, 250]), TensorShape([None, 250]))\n",
      "Denoising.build(): batch_input_shape =  (TensorShape([None, 250]), TensorShape([None, 250])) units =  250\n",
      "Denoising.call(): inputs =  (<tf.Tensor 'gaussian_noise_55/add_5:0' shape=(None, 250) dtype=float32>, <tf.Tensor 'dense_1007/BiasAdd:0' shape=(None, 250) dtype=float32>) self.a_1 =  <tf.Variable 'denoising_376/a_1:0' shape=(250,) dtype=float32>\n",
      "Denoising.build(): batch_input_shape =  (TensorShape([None, 250]), TensorShape([None, 250]))\n",
      "Denoising.build(): batch_input_shape =  (TensorShape([None, 250]), TensorShape([None, 250])) units =  250\n",
      "Denoising.call(): inputs =  (<tf.Tensor 'gaussian_noise_55/add_4:0' shape=(None, 250) dtype=float32>, <tf.Tensor 'dense_1006/BiasAdd:0' shape=(None, 250) dtype=float32>) self.a_1 =  <tf.Variable 'denoising_375/a_1:0' shape=(250,) dtype=float32>\n",
      "Denoising.build(): batch_input_shape =  (TensorShape([None, 250]), TensorShape([None, 250]))\n",
      "Denoising.build(): batch_input_shape =  (TensorShape([None, 250]), TensorShape([None, 250])) units =  250\n",
      "Denoising.call(): inputs =  (<tf.Tensor 'gaussian_noise_55/add_3:0' shape=(None, 250) dtype=float32>, <tf.Tensor 'dense_1005/BiasAdd:0' shape=(None, 250) dtype=float32>) self.a_1 =  <tf.Variable 'denoising_374/a_1:0' shape=(250,) dtype=float32>\n",
      "Denoising.build(): batch_input_shape =  (TensorShape([None, 500]), TensorShape([None, 500]))\n",
      "Denoising.build(): batch_input_shape =  (TensorShape([None, 500]), TensorShape([None, 500])) units =  500\n",
      "Denoising.call(): inputs =  (<tf.Tensor 'gaussian_noise_55/add_2:0' shape=(None, 500) dtype=float32>, <tf.Tensor 'dense_1004/BiasAdd:0' shape=(None, 500) dtype=float32>) self.a_1 =  <tf.Variable 'denoising_373/a_1:0' shape=(500,) dtype=float32>\n",
      "Denoising.build(): batch_input_shape =  (TensorShape([None, 1000]), TensorShape([None, 1000]))\n",
      "Denoising.build(): batch_input_shape =  (TensorShape([None, 1000]), TensorShape([None, 1000])) units =  1000\n",
      "Denoising.call(): inputs =  (<tf.Tensor 'gaussian_noise_55/add_1:0' shape=(None, 1000) dtype=float32>, <tf.Tensor 'dense_1003/BiasAdd:0' shape=(None, 1000) dtype=float32>) self.a_1 =  <tf.Variable 'denoising_372/a_1:0' shape=(1000,) dtype=float32>\n",
      "Denoising.build(): batch_input_shape =  (TensorShape([None, 784]), TensorShape([None, 784]))\n",
      "Denoising.build(): batch_input_shape =  (TensorShape([None, 784]), TensorShape([None, 784])) units =  784\n",
      "Denoising.call(): inputs =  (<tf.Tensor 'gaussian_noise_55/add:0' shape=(None, 784) dtype=float32>, <tf.Tensor 'dense_1002/BiasAdd:0' shape=(None, 784) dtype=float32>) self.a_1 =  <tf.Variable 'denoising_371/a_1:0' shape=(784,) dtype=float32>\n",
      "Denoising.call(): inputs =  (<tf.Tensor 'my_classifier4_55/batch_normalization_1035/batchnorm/add_1:0' shape=(None, 10) dtype=float32>, <tf.Tensor 'my_classifier4_55/Softmax_1:0' shape=(None, 10) dtype=float32>) self.a_1 =  <tf.Variable 'denoising_377/a_1:0' shape=(10,) dtype=float32>\n",
      "Denoising.call(): inputs =  (<tf.Tensor 'my_classifier4_55/gaussian_noise_55/add_5:0' shape=(None, 250) dtype=float32>, <tf.Tensor 'my_classifier4_55/dense_1007/BiasAdd:0' shape=(None, 250) dtype=float32>) self.a_1 =  <tf.Variable 'denoising_376/a_1:0' shape=(250,) dtype=float32>\n",
      "Denoising.call(): inputs =  (<tf.Tensor 'my_classifier4_55/gaussian_noise_55/add_4:0' shape=(None, 250) dtype=float32>, <tf.Tensor 'my_classifier4_55/dense_1006/BiasAdd:0' shape=(None, 250) dtype=float32>) self.a_1 =  <tf.Variable 'denoising_375/a_1:0' shape=(250,) dtype=float32>\n",
      "Denoising.call(): inputs =  (<tf.Tensor 'my_classifier4_55/gaussian_noise_55/add_3:0' shape=(None, 250) dtype=float32>, <tf.Tensor 'my_classifier4_55/dense_1005/BiasAdd:0' shape=(None, 250) dtype=float32>) self.a_1 =  <tf.Variable 'denoising_374/a_1:0' shape=(250,) dtype=float32>\n",
      "Denoising.call(): inputs =  (<tf.Tensor 'my_classifier4_55/gaussian_noise_55/add_2:0' shape=(None, 500) dtype=float32>, <tf.Tensor 'my_classifier4_55/dense_1004/BiasAdd:0' shape=(None, 500) dtype=float32>) self.a_1 =  <tf.Variable 'denoising_373/a_1:0' shape=(500,) dtype=float32>\n",
      "Denoising.call(): inputs =  (<tf.Tensor 'my_classifier4_55/gaussian_noise_55/add_1:0' shape=(None, 1000) dtype=float32>, <tf.Tensor 'my_classifier4_55/dense_1003/BiasAdd:0' shape=(None, 1000) dtype=float32>) self.a_1 =  <tf.Variable 'denoising_372/a_1:0' shape=(1000,) dtype=float32>\n",
      "Denoising.call(): inputs =  (<tf.Tensor 'my_classifier4_55/gaussian_noise_55/add:0' shape=(None, 784) dtype=float32>, <tf.Tensor 'my_classifier4_55/dense_1002/BiasAdd:0' shape=(None, 784) dtype=float32>) self.a_1 =  <tf.Variable 'denoising_371/a_1:0' shape=(784,) dtype=float32>\n",
      "Denoising.call(): inputs =  (<tf.Tensor 'my_classifier4_55/batch_normalization_1035/batchnorm/add_1:0' shape=(None, 10) dtype=float32>, <tf.Tensor 'my_classifier4_55/Softmax_1:0' shape=(None, 10) dtype=float32>) self.a_1 =  <tf.Variable 'denoising_377/a_1:0' shape=(10,) dtype=float32>\n",
      "Denoising.call(): inputs =  (<tf.Tensor 'my_classifier4_55/gaussian_noise_55/add_5:0' shape=(None, 250) dtype=float32>, <tf.Tensor 'my_classifier4_55/dense_1007/BiasAdd:0' shape=(None, 250) dtype=float32>) self.a_1 =  <tf.Variable 'denoising_376/a_1:0' shape=(250,) dtype=float32>\n",
      "Denoising.call(): inputs =  (<tf.Tensor 'my_classifier4_55/gaussian_noise_55/add_4:0' shape=(None, 250) dtype=float32>, <tf.Tensor 'my_classifier4_55/dense_1006/BiasAdd:0' shape=(None, 250) dtype=float32>) self.a_1 =  <tf.Variable 'denoising_375/a_1:0' shape=(250,) dtype=float32>\n",
      "Denoising.call(): inputs =  (<tf.Tensor 'my_classifier4_55/gaussian_noise_55/add_3:0' shape=(None, 250) dtype=float32>, <tf.Tensor 'my_classifier4_55/dense_1005/BiasAdd:0' shape=(None, 250) dtype=float32>) self.a_1 =  <tf.Variable 'denoising_374/a_1:0' shape=(250,) dtype=float32>\n",
      "Denoising.call(): inputs =  (<tf.Tensor 'my_classifier4_55/gaussian_noise_55/add_2:0' shape=(None, 500) dtype=float32>, <tf.Tensor 'my_classifier4_55/dense_1004/BiasAdd:0' shape=(None, 500) dtype=float32>) self.a_1 =  <tf.Variable 'denoising_373/a_1:0' shape=(500,) dtype=float32>\n",
      "Denoising.call(): inputs =  (<tf.Tensor 'my_classifier4_55/gaussian_noise_55/add_1:0' shape=(None, 1000) dtype=float32>, <tf.Tensor 'my_classifier4_55/dense_1003/BiasAdd:0' shape=(None, 1000) dtype=float32>) self.a_1 =  <tf.Variable 'denoising_372/a_1:0' shape=(1000,) dtype=float32>\n",
      "Denoising.call(): inputs =  (<tf.Tensor 'my_classifier4_55/gaussian_noise_55/add:0' shape=(None, 784) dtype=float32>, <tf.Tensor 'my_classifier4_55/dense_1002/BiasAdd:0' shape=(None, 784) dtype=float32>) self.a_1 =  <tf.Variable 'denoising_371/a_1:0' shape=(784,) dtype=float32>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - ETA: 0s - loss: 94.0272 - output_1_loss: 1.3513 - output_2_loss: 20.6910 - output_3_loss: 6.9500 - output_4_loss: 6.0077 - output_5_loss: 5.6202 - output_6_loss: 5.5284 - output_7_loss: 5.4796 - output_8_loss: 2.4467 - output_9_loss: 4.0542 - output_10_loss: 6.0984 - output_11_loss: 6.0925 - output_12_loss: 6.6275 - output_13_loss: 7.6244 - output_14_loss: 5.3717 - output_15_loss: 4.0835 - output_1_accuracy: 0.6124 - output_2_accuracy: 0.0000e+00 - output_3_accuracy: 0.0122 - output_4_accuracy: 0.0410 - output_5_accuracy: 0.0915 - output_6_accuracy: 0.0823 - output_7_accuracy: 0.0896 - output_8_accuracy: 0.6124 - output_9_accuracy: 0.4706 - output_10_accuracy: 0.0182 - output_11_accuracy: 0.0054 - output_12_accuracy: 0.0247 - output_13_accuracy: 0.0123 - output_14_accuracy: 0.0118 - output_15_accuracy: 0.3431           TA: 2:16 - loss: 127.1363 - output_1_loss: 1.8580 - output_2_loss: 20.6976 - output_3_loss: 9.2897 - output_4_loss: 7.9389 - output_5_loss: 8.1767 - output_6_loss: 8.0440 - output_7_loss: 8.0174 - output_8_loss: 4.7004 - output_9_loss: 6.6862 - output_10_loss: 7.9295 - output_11_loss: 7.8100 - output_12_loss: 10.0506 - output_13_loss: 10.4566 - output_14_loss: 8.4610 - output_15_loss: 7.0198 - output_1_accuracy: 0.4054 - output_2_accuracy: 0.0000e+00 - output_3_accuracy: 0.0034 - output_4_accuracy: 0.0191 - output_5_accuracy: 0.0374 - output_6_accuracy: 0.0354 - output_7_accuracy: 0.0406 - output_8_accuracy: 0.4054 - output_9_accurac - ETA: 1:57 - loss: 116.5186 - output_1_loss: 1.7125 - output_2_loss: 20.6955 - output_3_loss: 8.4732 - outpu - ETA: 1:17 - loss: 104.7924 - output_1_loss: 1.5477 - output_2_loss: 20.6929 - output_3_loss: 7.6341 - output_4_loss: 6.5737 - output_5_loss: 6.3582 - output_6_loss: 6.2584 - output_7_loss: 6.2341 - output_8_loss: 3.2323 - output_9_loss: 4.3244 - output_10_loss: 6.6580 - output_11_loss: 6.7154 - output_12_loss: 7.7880 - output_13_loss: 9.0421 - output_14_loss: 6.3557 - output_15_loss: 5.3776 - output_1_accuracy: 0.5329 - output_2_accuracy: 0.0000e+00 - output_3_accuracy: 0.0073 - output_4_accuracy: 0.0320 - output_5_accuracy: 0.0574 - output_6_accuracy: 0.0549 - output_7_accuracy: 0.0616 - output_8_accuracy: 0.5329 - output_9_accuracy: 0.4621 - output_10_accuracy: 0.0244 - output_11_accuracy: 9.3549e-04 - output_12_accuracy: 0.0197 - output_13_accuracy: 4.0706e-04 - out - ETA: 1:15 - loss: 104.2578 - output_1_loss: 1.5399 - output_2_loss: 20.6928 - output_3_loss: 7.5976 - output_4_loss: 6.5443 - output_5_loss: 6.3208 - output_6_loss: 6.2218 - output_7_loss: 6.1963 - output_8_loss: 3.1955 - output_9_loss: 4.2846 - output_10_loss: 6.6305 - output_11_loss: 6.6841 - output_12_loss: 7.7313 - output_13_loss: 8.9928 - output_14_loss: 6.3071 - output_15_loss: 5.3186 - output_1_accuracy: 0.5359 - output_2_accuracy: 0.0000e+00 - output_3_accuracy: 0.0075 - output_4_accuracy: 0.0323 - output_5_accuracy: 0.0582 - output_6_accuracy: 0.0557 - output_7_accuracy: 0.0625 - output_8_accuracy: 0.5359 - ETA: 1:06 - loss: 102.3626 - output_1_loss: 1.5112 - output_2_loss: 20.6924 - output_3_loss: 7.4693 - output_4_loss: 6.4404 - output_5_loss: 6.1889 - output_6_loss: 6.0916 - output_7_loss: 6.0621 - output_8_loss: 3.0627 - output_9_loss: 4.1626 - output_10_loss: 6.5334 - output_11_loss: 6.5726 - output_12_loss: 7.5307 - output_13_loss: 8.8053 - output_14_loss: 6.1361 - output_15_loss: 5.1033 - output_1_accuracy: 0.5472 - output_2_accuracy: 0.0000e+00 - output_3_accuracy: 0.0082 - output_4_accuracy: 0.0336 - output_5_accuracy: 0.0615 - output_6_accuracy: 0.0590 - output_7_accuracy: 0.0657 - output_8_accuracy: 0.5472 - output_9_accuracy: 0.4744 - output_10_ac - ETA: 58s - loss: 100.9779 - output_1_loss: 1.4886 - output_2_loss: 20.6921 - output_3_loss: 7.3773 - output_4_loss: 6.3654 - output_5_loss: 6.0925 - output_6_loss: 5.9966 - output_7_loss: 5.9642 - output_8_loss: 2.9628 - output_9_loss: 4.0978 - output_10_loss: 6.4629 - output_11_loss: 6.4918 - output_12_loss: 7.3842 - output_13_loss: 8.6490 - output_14_loss: 6.0124 - output_15_loss: 4.9405 - output_1_accuracy: 0.5563 - output_2_accuracy: 0.0000e+00 - output_3_accuracy: 0.0087 - output_4_accuracy: 0.0345 - output_5_accuracy: 0Denoising.call(): inputs =  (<tf.Tensor 'my_classifier4_55/batch_normalization_1035/batchnorm/add_1:0' shape=(None, 10) dtype=float32>, <tf.Tensor 'my_classifier4_55/Softmax_1:0' shape=(None, 10) dtype=float32>) self.a_1 =  <tf.Variable 'denoising_377/a_1:0' shape=(10,) dtype=float32>\n",
      "Denoising.call(): inputs =  (<tf.Tensor 'my_classifier4_55/batch_normalization_1034/batchnorm/add_1:0' shape=(None, 250) dtype=float32>, <tf.Tensor 'my_classifier4_55/dense_1007/BiasAdd:0' shape=(None, 250) dtype=float32>) self.a_1 =  <tf.Variable 'denoising_376/a_1:0' shape=(250,) dtype=float32>\n",
      "Denoising.call(): inputs =  (<tf.Tensor 'my_classifier4_55/batch_normalization_1033/batchnorm/add_1:0' shape=(None, 250) dtype=float32>, <tf.Tensor 'my_classifier4_55/dense_1006/BiasAdd:0' shape=(None, 250) dtype=float32>) self.a_1 =  <tf.Variable 'denoising_375/a_1:0' shape=(250,) dtype=float32>\n",
      "Denoising.call(): inputs =  (<tf.Tensor 'my_classifier4_55/batch_normalization_1032/batchnorm/add_1:0' shape=(None, 250) dtype=float32>, <tf.Tensor 'my_classifier4_55/dense_1005/BiasAdd:0' shape=(None, 250) dtype=float32>) self.a_1 =  <tf.Variable 'denoising_374/a_1:0' shape=(250,) dtype=float32>\n",
      "Denoising.call(): inputs =  (<tf.Tensor 'my_classifier4_55/batch_normalization_1031/batchnorm/add_1:0' shape=(None, 500) dtype=float32>, <tf.Tensor 'my_classifier4_55/dense_1004/BiasAdd:0' shape=(None, 500) dtype=float32>) self.a_1 =  <tf.Variable 'denoising_373/a_1:0' shape=(500,) dtype=float32>\n",
      "Denoising.call(): inputs =  (<tf.Tensor 'my_classifier4_55/batch_normalization_1030/batchnorm/add_1:0' shape=(None, 1000) dtype=float32>, <tf.Tensor 'my_classifier4_55/dense_1003/BiasAdd:0' shape=(None, 1000) dtype=float32>) self.a_1 =  <tf.Variable 'denoising_372/a_1:0' shape=(1000,) dtype=float32>\n",
      "Denoising.call(): inputs =  (<tf.Tensor 'my_classifier4_55/flatten_111/Reshape:0' shape=(None, 784) dtype=float32>, <tf.Tensor 'my_classifier4_55/dense_1002/BiasAdd:0' shape=(None, 784) dtype=float32>) self.a_1 =  <tf.Variable 'denoising_371/a_1:0' shape=(784,) dtype=float32>\n",
      "1563/1563 [==============================] - 181s 103ms/step - loss: 94.0194 - output_1_loss: 1.3512 - output_2_loss: 20.6910 - output_3_loss: 6.9496 - output_4_loss: 6.0073 - output_5_loss: 5.6197 - output_6_loss: 5.5279 - output_7_loss: 5.4791 - output_8_loss: 2.4461 - output_9_loss: 4.0543 - output_10_loss: 6.0980 - output_11_loss: 6.0921 - output_12_loss: 6.6266 - output_13_loss: 7.6231 - output_14_loss: 5.3710 - output_15_loss: 4.0825 - output_1_accuracy: 0.6124 - output_2_accuracy: 0.0000e+00 - output_3_accuracy: 0.0122 - output_4_accuracy: 0.0410 - output_5_accuracy: 0.0915 - output_6_accuracy: 0.0824 - output_7_accuracy: 0.0896 - output_8_accuracy: 0.6124 - output_9_accuracy: 0.4706 - output_10_accuracy: 0.0182 - output_11_accuracy: 0.0054 - output_12_accuracy: 0.0247 - output_13_accuracy: 0.0124 - output_14_accuracy: 0.0118 - output_15_accuracy: 0.3432 - val_loss: 71.6604 - val_output_1_loss: 0.8082 - val_output_2_loss: 20.6930 - val_output_3_loss: 5.8548 - val_output_4_loss: 5.0176 - val_output_5_loss: 4.9192 - val_output_6_loss: 4.1992 - val_output_7_loss: 3.9668 - val_output_8_loss: 0.9614 - val_output_9_loss: 5.1385 - val_output_10_loss: 4.7625 - val_output_11_loss: 4.6965 - val_output_12_loss: 3.7650 - val_output_13_loss: 3.0063 - val_output_14_loss: 2.6149 - val_output_15_loss: 1.2566 - val_output_1_accuracy: 0.8527 - val_output_2_accuracy: 0.0000e+00 - val_output_3_accuracy: 0.0299 - val_output_4_accuracy: 0.0637 - val_output_5_accuracy: 0.2758 - val_output_6_accuracy: 0.3483 - val_output_7_accuracy: 0.2840 - val_output_8_accuracy: 0.8527 - val_output_9_accuracy: 0.2933 - val_output_10_accuracy: 0.0000e+00 - val_output_11_accuracy: 0.0693 - val_output_12_accuracy: 0.2828 - val_output_13_accuracy: 0.4473 - val_output_14_accuracy: 0.2005 - val_output_15_accuracy: 0.6993\n"
     ]
    }
   ],
   "source": [
    "my4_model = MyClassifier4()\n",
    "#my4_model.compile(loss = lossfunc_ladder_supervised, optimizer = \"Adam\", metrics = [\"accuracy\"])\n",
    "my4_model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"Adam\", metrics = [\"accuracy\"])\n",
    "history = my4_model.fit(X_train, y_train, epochs = 1, validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
